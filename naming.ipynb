{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 1: expected str instance, NoneType found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 68\u001b[0m\n\u001b[0;32m     64\u001b[0m         shutil\u001b[38;5;241m.\u001b[39mrmtree(tmp)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 68\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[22], line 62\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 62\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m     shutil\u001b[38;5;241m.\u001b[39mrmtree(tmp)\n",
      "Cell \u001b[1;32mIn[22], line 43\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# ... (rest of your code remains unchanged)\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# List the files in the most recent year\u001b[39;00m\n\u001b[0;32m     42\u001b[0m processing_year \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m files_base_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mftp_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessing_year\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m files_base_ls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(__list_ftp_dir(files_base_url))\n\u001b[0;32m     45\u001b[0m _zip \u001b[38;5;241m=\u001b[39m [file \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files_base_ls \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n",
      "\u001b[1;31mTypeError\u001b[0m: sequence item 1: expected str instance, NoneType found"
     ]
    }
   ],
   "source": [
    "from urllib import request\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    " \n",
    "# ... (other imports)\n",
    "import sys\n",
    "import os \n",
    "import json\n",
    "import shlex\n",
    "import logging\n",
    "import subprocess\n",
    "import shutil\n",
    "import zipfile\n",
    "from threading import Timer\n",
    "import bs4\n",
    "import pandas as pd\n",
    "def __list_ftp_dir(url):\n",
    "    with request.urlopen(url) as response:\n",
    "        soup = bs4.BeautifulSoup(response, features=\"html.parser\")\n",
    "        for filename in soup.find_all(\"a\"):\n",
    "            yield filename.text\n",
    " \n",
    "def __download_file(url, zip_output):\n",
    "    with request.urlopen(url) as response, open(zip_output, 'wb') as out_file:\n",
    "        shutil.copyfileobj(response, out_file)\n",
    " \n",
    "def main(**kwargs):\n",
    "\n",
    " \n",
    "    ftp_url = \"ftp://geoftp.ibge.gov.br/organizacao_do_territorio/estrutura_territorial/divisao_territorial\"\n",
    "    key_name = \"org_raw_estrutura_territorial\"\n",
    "    tmp = \"/tmp/org_raw_estrutura_territorial/\"\n",
    " \n",
    "    # ... (rest of your code remains unchanged)\n",
    " \n",
    "    try:\n",
    "        os.makedirs(tmp, mode=0o777, exist_ok=True)\n",
    " \n",
    "        # ... (rest of your code remains unchanged)\n",
    " \n",
    "        # List the files in the most recent year\n",
    "        processing_year = None\n",
    "        files_base_url = \"/\".join([ftp_url, processing_year])\n",
    "        files_base_ls = list(__list_ftp_dir(files_base_url))\n",
    "        _zip = [file for file in files_base_ls if file.endswith(\".zip\")]\n",
    "        if len(_zip) > 0:\n",
    "            _zip = _zip[0]\n",
    " \n",
    "        zip_output = os.path.join(tmp, _zip)\n",
    "        # Download the zip file\n",
    "        __download_file(\"/\".join([files_base_url, _zip]), zip_output)\n",
    "\n",
    "        xls_output = None\n",
    "        with zipfile.ZipFile(zip_output) as _zip:\n",
    "            zip_files = _zip.filelist\n",
    "            for zip_file in zip_files:\n",
    "                if zip_file.filename.lower().endswith(\"municipio.xls\"):\n",
    "                    xls_output = _zip.extract(zip_file, tmp)\n",
    "                    break\n",
    "\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "    finally:\n",
    "        shutil.rmtree(tmp)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 1: expected str instance, NoneType found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 157\u001b[0m\n\u001b[0;32m    153\u001b[0m         shutil\u001b[38;5;241m.\u001b[39mrmtree(tmp)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 157\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 151\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28mprint\u001b[39m(files_base_ls)\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    153\u001b[0m     shutil\u001b[38;5;241m.\u001b[39mrmtree(tmp)\n",
      "Cell \u001b[1;32mIn[20], line 147\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    122\u001b[0m processing_year \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m#if kwargs[\"reload\"] is False:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m#    if call_redis(\"exists\", key_name):\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m#        processing_year = call_redis(\"get\", key_name).decode(\"utf-8\")\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    145\u001b[0m \n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# List the files in the most recent year\u001b[39;00m\n\u001b[1;32m--> 147\u001b[0m files_base_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessing_year\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m files_base_ls \u001b[38;5;241m=\u001b[39m __list_ftp_dir(files_base_url)\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28mprint\u001b[39m(files_base_ls)\n",
      "\u001b[1;31mTypeError\u001b[0m: sequence item 1: expected str instance, NoneType found"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os \n",
    "import json\n",
    "import shlex\n",
    "import logging\n",
    "import subprocess\n",
    "import shutil\n",
    "import zipfile\n",
    "from threading import Timer\n",
    "import bs4\n",
    "import pandas as pd\n",
    "\n",
    "########################################################\n",
    "import re\n",
    "import os\n",
    "import subprocess\n",
    "import shlex\n",
    "from unicodedata import normalize\n",
    "\n",
    "\n",
    "def normalize_string(_str):\n",
    "    return re.sub(\n",
    "        r'[,;:{}()\\n\\t=*%]',\n",
    "        \"\",\n",
    "        normalize(\"NFKD\", _str)\n",
    "        .encode(\"ASCII\", \"ignore\")\n",
    "        .decode(\"ASCII\")\n",
    "        .replace(\" \", \"-\")\n",
    "        .replace(\"-\", \"_\")\n",
    "        .lower(),\n",
    "    )\n",
    "\n",
    "\n",
    "def normalize_replace(string: str) -> str:\n",
    "    return re.sub(\n",
    "        r\"[,;:{}()\\n\\t=*%]\",\n",
    "        \"\",\n",
    "        normalize_string(string)\n",
    "        .replace(\"  \", \"_\")\n",
    "        .replace(\" \", \"_\")\n",
    "        .replace(\"-\", \"_\")\n",
    "        .replace(\"|\", \"_\")\n",
    "        .replace(\"/\", \"_\")\n",
    "        .replace(\".\", \"_\")\n",
    "        .upper(),\n",
    "    )\n",
    "\n",
    "def change_encoding(file, DEBUG=False):\n",
    "    p1 = subprocess.Popen(shlex.split('file -bi \"%s\"' % file), stdout=subprocess.PIPE)\n",
    "    p2 = subprocess.Popen(\n",
    "        shlex.split(\"awk -F '=' '{print $2}'\"), stdin=p1.stdout, stdout=subprocess.PIPE\n",
    "    )\n",
    "    encoding = p2.communicate()[0].decode(\"utf-8\").strip(\"\\n\")\n",
    "\n",
    "    # For dev purposes\n",
    "    if DEBUG:\n",
    "        os.system('mv \"{file}\" \"{file}.old\"'.format(file=file))\n",
    "        os.system('head -n 500 \"{file}.old\" > \"{file}\"'.format(file=file))\n",
    "\n",
    "    if encoding not in [\"utf-8\", \"us-ascii\"]:\n",
    "        os.system('mv \"{file}\" \"{file}.old\"'.format(file=file))\n",
    "        subprocess.Popen(\n",
    "            shlex.split(\n",
    "                'iconv -f {f} -t utf-8 \"{file}.old\" -o \"{output}\"'.format(\n",
    "                    f=encoding, file=file, output=file\n",
    "                )\n",
    "            )\n",
    "        ).communicate()\n",
    "\n",
    "\n",
    "########################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def __list_ftp_dir(url):\n",
    "    cmd = \"wget --progress=bar:force --no-check-certificate -O- -e use_proxy=yes {url}\".format(url=url.replace('ftp://',''))\n",
    "    stdin, stdout = subprocess.Popen(\n",
    "        cmd, shell=True, stdin=subprocess.PIPE, stdout=subprocess.PIPE\n",
    "    ).communicate()\n",
    "    soup = bs4.BeautifulSoup(stdin, features=\"html.parser\")\n",
    "    if len(soup) == 0:\n",
    "        pass\n",
    "        #raise Exception(\"Something bad is occurring\")\n",
    "\n",
    "    for filename in soup.find_all(\"a\"):\n",
    "        yield filename.text\n",
    "\n",
    "\n",
    "def __download_file(url, zip_output):\n",
    "    cmd = \"wget --progress=bar:force --no-check-certificate -e use_proxy=yes {url} -O {output}\".format(\n",
    "        url=url, output=zip_output\n",
    "    )\n",
    "    process = subprocess.Popen(\n",
    "        shlex.split(cmd), stdin=subprocess.PIPE, stdout=subprocess.PIPE\n",
    "    )\n",
    "    timer = Timer(3600, process.kill)\n",
    "    try:\n",
    "        timer.start()\n",
    "        process.communicate()\n",
    "    finally:\n",
    "        timer.cancel()\n",
    "\n",
    "\n",
    "def main():\n",
    "    #bot = initialize()\n",
    "    #LND: str = bot.lnd\n",
    "    #adl: FileSystemClient = bot.adl\n",
    "\n",
    "    url = \"ftp://geoftp.ibge.gov.br/organizacao_do_territorio/estrutura_territorial/divisao_territorial\"\n",
    "    key_name = \"org_raw_estrutura_territorial\"\n",
    "    tmp = \"/tmp/org_raw_estrutura_territorial/\"\n",
    "\n",
    "    try:\n",
    "        os.makedirs(tmp, mode=0o777, exist_ok=True)\n",
    "\n",
    "        #if kwargs[\"reset\"] is True:\n",
    "        #    call_redis(\"delete\", key_name)\n",
    "\n",
    "        #ls = list(__list_ftp_dir(url))\n",
    "\n",
    "        processing_year = None\n",
    "        #if kwargs[\"reload\"] is False:\n",
    "        #    if call_redis(\"exists\", key_name):\n",
    "        #        processing_year = call_redis(\"get\", key_name).decode(\"utf-8\")\n",
    "\n",
    "            # If redis cached year is the last available year 'ls[-1]'\n",
    "        #    if processing_year is None or processing_year != ls[-1]:\n",
    "        #        processing_year = ls[-1]\n",
    "        #    else:\n",
    "        #        log_status(\"no_new_file\")\n",
    "        #        sys.exit(0)\n",
    "        #else:\n",
    "        #    processing_year = kwargs[\"reload\"]\n",
    "        #    if processing_year not in ls:\n",
    "                \n",
    "        #        logging.warning(json.dumps(\n",
    "        #            {\n",
    "        #                \"exit\": 404,\n",
    "        #                \"msg\": \"%s not found\" % processing_year,\n",
    "        #                \"available_years\": ls,\n",
    "        #            }\n",
    "        #        ))\n",
    "        #        sys.exit(0)\n",
    "\n",
    "        # List the files in the most recent year\n",
    "        files_base_url = \"/\".join([url, processing_year])\n",
    "        files_base_ls = __list_ftp_dir(files_base_url)\n",
    "        print(files_base_ls)\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "    finally:\n",
    "        shutil.rmtree(tmp)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dsadasdasda\n"
     ]
    }
   ],
   "source": [
    "print('dsadasdasda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os \n",
    "import json\n",
    "import shlex\n",
    "import logging\n",
    "import subprocess\n",
    "import shutil\n",
    "import zipfile\n",
    "from threading import Timer\n",
    "import bs4\n",
    "import pandas as pd\n",
    "\n",
    "from azure.storage.filedatalake import FileSystemClient\n",
    "\n",
    "from core.bot import initialize, log_status\n",
    "from core.redis import call_redis\n",
    "from core.string_utils import normalize_string\n",
    "from core.adls import drop_directory, upload_file\n",
    "\n",
    "\n",
    "def __list_ftp_dir(url):\n",
    "    cmd = \"wget --progress=bar:force --no-check-certificate -O- -e use_proxy=yes {url}\".format(url=url.replace('ftp://',''))\n",
    "    stdin, stdout = subprocess.Popen(\n",
    "        cmd, shell=True, stdin=subprocess.PIPE, stdout=subprocess.PIPE\n",
    "    ).communicate()\n",
    "    soup = bs4.BeautifulSoup(stdin, features=\"html.parser\")\n",
    "    if len(soup) == 0:\n",
    "        pass\n",
    "        #raise Exception(\"Something bad is occurring\")\n",
    "\n",
    "    for filename in soup.find_all(\"a\"):\n",
    "        yield filename.text\n",
    "\n",
    "\n",
    "def __download_file(url, zip_output):\n",
    "    cmd = \"wget --progress=bar:force --no-check-certificate -e use_proxy=yes {url} -O {output}\".format(\n",
    "        url=url, output=zip_output\n",
    "    )\n",
    "    process = subprocess.Popen(\n",
    "        shlex.split(cmd), stdin=subprocess.PIPE, stdout=subprocess.PIPE\n",
    "    )\n",
    "    timer = Timer(3600, process.kill)\n",
    "    try:\n",
    "        timer.start()\n",
    "        process.communicate()\n",
    "    finally:\n",
    "        timer.cancel()\n",
    "\n",
    "\n",
    "def main(**kwargs):\n",
    "    bot = initialize()\n",
    "    LND: str = bot.lnd\n",
    "    adl: FileSystemClient = bot.adl\n",
    "\n",
    "    url = \"ftp://geoftp.ibge.gov.br/organizacao_do_territorio/estrutura_territorial/divisao_territorial\"\n",
    "    key_name = \"org_raw_estrutura_territorial\"\n",
    "    tmp = \"/tmp/org_raw_estrutura_territorial/\"\n",
    "\n",
    "    try:\n",
    "        os.makedirs(tmp, mode=0o777, exist_ok=True)\n",
    "\n",
    "        if kwargs[\"reset\"] is True:\n",
    "            call_redis(\"delete\", key_name)\n",
    "\n",
    "        ls = list(__list_ftp_dir(url))\n",
    "\n",
    "        processing_year = None\n",
    "        if kwargs[\"reload\"] is False:\n",
    "            if call_redis(\"exists\", key_name):\n",
    "                processing_year = call_redis(\"get\", key_name).decode(\"utf-8\")\n",
    "\n",
    "            # If redis cached year is the last available year 'ls[-1]'\n",
    "            if processing_year is None or processing_year != ls[-1]:\n",
    "                processing_year = ls[-1]\n",
    "            else:\n",
    "                log_status(\"no_new_file\")\n",
    "                sys.exit(0)\n",
    "        else:\n",
    "            processing_year = kwargs[\"reload\"]\n",
    "            if processing_year not in ls:\n",
    "                \n",
    "                logging.warning(json.dumps(\n",
    "                    {\n",
    "                        \"exit\": 404,\n",
    "                        \"msg\": \"%s not found\" % processing_year,\n",
    "                        \"available_years\": ls,\n",
    "                    }\n",
    "                ))\n",
    "                sys.exit(0)\n",
    "\n",
    "        # List the files in the most recent year\n",
    "        files_base_url = \"/\".join([url, processing_year])\n",
    "        files_base_ls = __list_ftp_dir(files_base_url)\n",
    "        _zip = [file for file in files_base_ls if file.endswith(\".zip\")]\n",
    "        if len(_zip) > 0:\n",
    "            _zip = _zip[0]\n",
    "\n",
    "        zip_output = tmp + _zip\n",
    "        # Download the zip file\n",
    "        __download_file(\"/\".join([files_base_url, _zip]), zip_output)\n",
    "\n",
    "        xls_output = None\n",
    "        with zipfile.ZipFile(zip_output) as _zip:\n",
    "            zip_files = _zip.filelist\n",
    "            for zip_file in zip_files:\n",
    "                if zip_file.filename.lower().endswith(\"municipio.xls\"):\n",
    "                    xls_output = _zip.extract(zip_file, tmp)\n",
    "                    break\n",
    "\n",
    "        df = pd.read_excel(xls_output, index_col=False)\n",
    "        df.columns = [normalize_string(col) for col in df.columns]\n",
    "\n",
    "        filename, ext = os.path.basename(xls_output).split(\".\")\n",
    "        ext = ext.lower()\n",
    "        filename = f\"{filename}_{processing_year.replace('/','')}.{ext}.parquet\"\n",
    "\n",
    "        parquet_output = tmp + filename\n",
    "        df.to_parquet(parquet_output, index=False)\n",
    "\n",
    "        schema, table = \"ibge\", \"relatorio_dtb_brasil_municipio\"\n",
    "        drop_directory(LND, adl, schema, table)\n",
    "        upload_file(LND, adl, schema, table, parquet_output)\n",
    "\n",
    "        if kwargs[\"reload\"] is False:\n",
    "            call_redis(\"set\", key_name, processing_year)\n",
    "        log_status(\"ok\")\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "    finally:\n",
    "        shutil.rmtree(tmp)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(**kwargs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_bot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
