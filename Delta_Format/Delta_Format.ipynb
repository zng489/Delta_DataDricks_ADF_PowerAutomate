{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fabf108d-c3bb-468a-9980-0cbe7eefcb00",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Se precisar\n",
    "# python\n",
    "# Copiar código\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Inicializar Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaLakeDetailedExample\") \\\n",
    "    .config(\"spark.sql.extensions\", \"delta.sql.DeltaSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ecea8b9-8f6c-4f05-b562-50e125e46984",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "### Importar bibliotecas necessárias ###\n",
    "########################################\n",
    "\n",
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Inicializar a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaLakeExample\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Criar um DataFrame de exemplo\n",
    "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Cathy\", 29)]\n",
    "columns = [\"name\", \"age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Salvar como uma tabela Delta\n",
    "#df.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/delta_table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3b6b063-2154-46ef-b626-8dd95be16dcf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>age</th></tr></thead><tbody><tr><td>Alice</td><td>34</td></tr><tr><td>Bob</td><td>45</td></tr><tr><td>Cathy</td><td>29</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Alice",
         34
        ],
        [
         "Bob",
         45
        ],
        [
         "Cathy",
         29
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "752ebbee-42d7-4d95-8f70-3c6250cf0157",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[5]: []"
     ]
    }
   ],
   "source": [
    "dbutils.fs.ls(\"dbfs:/Folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2b28fb0-69f6-4ca2-a385-02beae4f21bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[4]: True"
     ]
    }
   ],
   "source": [
    "# Make a Directory:\n",
    "dbutils.fs.mkdirs(\"dbfs:/Folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ee36c91-d1c1-4424-bb7d-724eccd3ea0f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "####################################\n",
    "### Salvar como uma tabela Delta ###\n",
    "####################################\n",
    "\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/Folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "889a704e-1aca-4bcb-9dfc-40042e174107",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n| name|age|\n+-----+---+\n|Alice| 34|\n|Cathy| 29|\n|  Bob| 45|\n+-----+---+\n\n"
     ]
    }
   ],
   "source": [
    "#################################\n",
    "### Ler dados da tabela Delta ###\n",
    "#################################\n",
    "\n",
    "df_delta = spark.read.format(\"delta\").load(\"dbfs:/Folder\")\n",
    "df_delta.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88e19ffa-e536-4327-b019-4999fae77b74",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "### Criar um novo DataFrame para atualização ###\n",
    "################################################\n",
    "\n",
    "new_data = [(\"Alice\", 35), (\"Bob\", 45), (\"David\", 28)]\n",
    "new_df = spark.createDataFrame(new_data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "262b5e9a-50d3-4a16-a27e-105feb2be17c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>age</th></tr></thead><tbody><tr><td>Alice</td><td>35</td></tr><tr><td>Bob</td><td>45</td></tr><tr><td>David</td><td>28</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Alice",
         35
        ],
        [
         "Bob",
         45
        ],
        [
         "David",
         28
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31120058-a5fa-43c0-a32a-7bfd57258d65",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "### Criar um novo DataFrame para atualização ###\n",
    "################################################\n",
    "\n",
    "# new_data = [(\"Alice\", 35), (\"Bob\", 45), (\"David\", 28)]\n",
    "# new_df = spark.createDataFrame(new_data, columns)\n",
    "\n",
    "# Atualizar os dados existentes\n",
    "# new_df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(\"/mnt/delta_table\")\n",
    "new_df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(\"dbfs:/Folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f1fe245-f28b-416c-bdbe-e2331acfdfdd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d2c554f-4a4f-4860-8a0f-3acd7022c398",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>age</th></tr></thead><tbody><tr><td>Alice</td><td>35</td></tr><tr><td>David</td><td>28</td></tr><tr><td>Bob</td><td>45</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Alice",
         35
        ],
        [
         "David",
         28
        ],
        [
         "Bob",
         45
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(\"dbfs:/Folder\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f484811-4591-4ddd-81bb-cda2dd95bc81",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: string (nullable = true)\n |-- age: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(\"dbfs:/Folder\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63281f7a-dbcd-42c6-a19d-59e6dd6c861d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a82887b-43ba-4537-82c5-9ba4bccc04b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n| name|age|\n+-----+---+\n|Alice| 34|\n|Cathy| 29|\n|  Bob| 45|\n+-----+---+\n\n"
     ]
    }
   ],
   "source": [
    "###############################################\n",
    "### Consultar uma versão anterior da tabela ###\n",
    "###############################################\n",
    "\n",
    "# version_df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"/mnt/delta_table\")\n",
    "version_df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"dbfs:/Folder\")\n",
    "version_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfcc4089-773f-4f22-bfe0-c5c0413fe9c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n| name|age|\n+-----+---+\n|Alice| 35|\n|David| 28|\n|  Bob| 45|\n+-----+---+\n\n"
     ]
    }
   ],
   "source": [
    "version_df = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(\"dbfs:/Folder\")\n",
    "version_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb52c914-295d-4be8-a9a5-f1c3a3ad4672",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3307471690888716>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m version_df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mversionAsOf\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m2\u001B[39m)\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdbfs:/Folder\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      2\u001B[0m version_df\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:302\u001B[0m, in \u001B[0;36mDataFrameReader.load\u001B[0;34m(self, path, format, schema, **options)\u001B[0m\n",
       "\u001B[1;32m    300\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions)\n",
       "\u001B[1;32m    301\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path, \u001B[38;5;28mstr\u001B[39m):\n",
       "\u001B[0;32m--> 302\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jreader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m)\n",
       "\u001B[1;32m    303\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m    304\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(path) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlist\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: Cannot time travel Delta table to version 2. Available versions: [0, 1]."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-3307471690888716>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m version_df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mversionAsOf\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m2\u001B[39m)\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdbfs:/Folder\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      2\u001B[0m version_df\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:302\u001B[0m, in \u001B[0;36mDataFrameReader.load\u001B[0;34m(self, path, format, schema, **options)\u001B[0m\n\u001B[1;32m    300\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions)\n\u001B[1;32m    301\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m--> 302\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jreader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    303\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    304\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(path) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlist\u001B[39m:\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: Cannot time travel Delta table to version 2. Available versions: [0, 1].",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Cannot time travel Delta table to version 2. Available versions: [0, 1].",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "version_df = spark.read.format(\"delta\").option(\"versionAsOf\", 2).load(\"dbfs:/Folder\")\n",
    "version_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4036884c-96b8-4c04-b6a6-6e319b1f21d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6b3ca86-d239-4380-a05e-dd7e24d90670",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#################################\n",
    "### Atualizar Dados com Merge ###\n",
    "#################################\n",
    "\n",
    "# Suponha que queremos atualizar a idade de \"Alice\" e adicionar um novo registro:\n",
    "  \n",
    "# Novo DataFrame para atualização\n",
    "updates = [(\"Alice\", 40), (\"David\", 28)]\n",
    "updates_df = spark.createDataFrame(updates, columns)\n",
    "\n",
    "# Usar Merge para atualizar e inserir dados\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Criar uma tabela Delta a partir do caminho existente\n",
    "# delta_table = DeltaTable.forPath(spark, \"/mnt/delta_table\")\n",
    "delta_table = DeltaTable.forPath(spark, \"dbfs:/Folder\")\n",
    "\n",
    "\n",
    "# Fazer o merge\n",
    "delta_table.alias(\"old_data\") \\\n",
    "    .merge(\n",
    "        updates_df.alias(\"new_data\"),\n",
    "        \"old_data.name = new_data.name\"\n",
    "    ) \\\n",
    "    .whenMatchedUpdate(set={\"age\": \"new_data.age\"}) \\\n",
    "    .whenNotMatchedInsert(values={\"name\": \"new_data.name\", \"age\": \"new_data.age\"}) \\\n",
    "    .execute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e32f821-9dcb-422b-8bbf-859a7271d81d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n| name|age|\n+-----+---+\n|Alice| 40|\n|David| 28|\n|  Bob| 45|\n+-----+---+\n\n"
     ]
    }
   ],
   "source": [
    "delta_table.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45110c48-356c-490b-8f62-9b19fe0159a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbd4bde0-1778-4eaa-9af9-93846330215f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n| name|age|\n+-----+---+\n|Alice| 40|\n|  Bob| 45|\n+-----+---+\n\n+-------+-----+------------------+\n|summary| name|               age|\n+-------+-----+------------------+\n|  count|    3|                 3|\n|   mean| null|37.666666666666664|\n| stddev| null| 8.736894948054104|\n|    min|Alice|                28|\n|    max|David|                45|\n+-------+-----+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "### Ler dados filtrados ###\n",
    "###########################\n",
    "\n",
    "filtered_df = spark.read.format(\"delta\").load(\"dbfs:/Folder\").filter(\"age > 30\")\n",
    "filtered_df.show()\n",
    "# Exibir estatísticas da tabela Delta\n",
    "delta_table.toDF().describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00d35cc7-3cb0-4a87-94f4-c21d695766f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c34d962-76d6-4d53-b5e9-4e336fb449d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#################################\n",
    "### Otimização com Z-Ordering ###\n",
    "################################\n",
    "\n",
    "# Podemos melhorar o desempenho das consultas usando Z-Ordering:\n",
    "\n",
    "# python\n",
    "# Copiar código\n",
    "# Otimizar a tabela com Z-Ordering\n",
    "delta_table.optimize().where(\"age > 30\").execute()\n",
    "\n",
    "# Optimize the table with Z-Ordering\n",
    "delta_table.optimize().zOrderBy(\"age\").execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f35d36af-34ad-41f6-99a6-24efbc9f92e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c1774fb-ed6e-4cab-93cf-03c902480265",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[24]: DataFrame[path: string]"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "### Compactar arquivos pequenos ###\n",
    "###################################\n",
    "\n",
    "### spark.sql(\"OPTIMIZE delta.`/mnt/delta_table`\")\n",
    "\n",
    "# Remover arquivos antigos que não são mais necessários\n",
    "### spark.sql(\"VACUUM delta.`/mnt/delta_table` RETAIN 168 HOURS\")  # Retém arquivos por 7 dias\n",
    "spark.sql(\"VACUUM delta.`dbfs:/Folder` RETAIN 168 HOURS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6802ff72-4059-45b4-9b8c-3bcdaeffab94",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th></tr></thead><tbody><tr><td>dbfs:/Folder</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/Folder"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.sql(\"VACUUM delta.`dbfs:/Folder` RETAIN 168 HOURS\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0908bf38-ed3d-4b2d-ad3b-6eb00d0917f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69ff1976-e52e-4c4e-8816-90ac19c89d10",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "########################\n",
    "### Schema Evolution ###\n",
    "########################\n",
    "\n",
    "# O Delta Lake permite que você altere o esquema da tabela, adicionando novas colunas conforme necessário.\n",
    "# python\n",
    "# Copiar código\n",
    "# Adicionar uma nova coluna\n",
    "\n",
    "new_data = [(\"Alice\", 35, \"F\"), (\"Bob\", 45, \"M\"), (\"Cathy\", 29, \"F\")]\n",
    "new_columns = [\"name\", \"age\", \"gender\"]\n",
    "new_df = spark.createDataFrame(new_data, new_columns)\n",
    "\n",
    "# Salvar com evolução de esquema\n",
    "\n",
    "# new_df.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").save(\"/mnt/delta_table\")\n",
    "new_df.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").save(\"dbfs:/Folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f0806a3-54e4-4d7d-869e-8c4ea4fac1da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>age</th><th>gender</th></tr></thead><tbody><tr><td>Alice</td><td>35</td><td>F</td></tr><tr><td>Cathy</td><td>29</td><td>F</td></tr><tr><td>Bob</td><td>45</td><td>M</td></tr><tr><td>Alice</td><td>40</td><td>null</td></tr><tr><td>David</td><td>28</td><td>null</td></tr><tr><td>Bob</td><td>45</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Alice",
         35,
         "F"
        ],
        [
         "Cathy",
         29,
         "F"
        ],
        [
         "Bob",
         45,
         "M"
        ],
        [
         "Alice",
         40,
         null
        ],
        [
         "David",
         28,
         null
        ],
        [
         "Bob",
         45,
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "gender",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(\"dbfs:/Folder\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6451358-d12e-48ce-97bf-bc7cfad06ad6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2db8ed5-48f3-4e9a-8725-1d289da09def",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#######################################################\n",
    "### Gerenciamento de Conflitos de Leitura e Escrita ###\n",
    "#######################################################\n",
    "\n",
    "# O Delta Lake permite que você trate conflitos de forma eficiente, usando a funcionalidade de transações ACID.\n",
    "# python\n",
    "# Copiar código\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Tentar atualizar a tabela enquanto lê\n",
    "delta_table = DeltaTable.forPath(spark, \"/mnt/delta_table\")\n",
    "try:\n",
    "    # Exemplo de operação de leitura e escrita simultânea\n",
    "    df_current = delta_table.toDF()\n",
    "    df_current.show()\n",
    "\n",
    "    # Atualizar a tabela\n",
    "    delta_table.update(\"name = 'Alice'\", {\"age\": \"36\"})\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebd21166-4a44-4ebf-8624-f4782519eb9c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "453eb8a3-a659-4397-a667-deee9917d6db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#######################\n",
    "### Particionamento ###\n",
    "#######################\n",
    "\n",
    "# Particionar tabelas Delta pode melhorar o desempenho das consultas.\n",
    "# python\n",
    "# Copiar código\n",
    "# Salvar como tabela Delta particionada\n",
    "\n",
    "df.write.partitionBy(\"age\").format(\"delta\").mode(\"overwrite\").save(\"/mnt/delta_table_partitioned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ddfa66a4-a083-4d1c-aecc-aac46896081d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60fc2c50-acfb-4031-84b4-8bff5b60c1a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------------+--------------------------+------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n|version|timestamp          |userId          |userName                  |operation   |operationParameters                                                                                                                                                                  |job |notebook          |clusterId           |readVersion|isolationLevel   |isBlindAppend|operationMetrics                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |userMetadata|engineInfo                         |\n+-------+-------------------+----------------+--------------------------+------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n|7      |2024-10-25 23:06:52|3466497405212432|zhang.yuan@senaicni.com.br|WRITE       |{mode -> Append, partitionBy -> []}                                                                                                                                                  |null|{3307471690888693}|0429-204748-rasps399|6          |WriteSerializable|true         |{numFiles -> 3, numOutputRows -> 3, numOutputBytes -> 3064}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |null        |Databricks-Runtime/12.2.x-scala2.12|\n|6      |2024-10-25 23:05:33|3466497405212432|zhang.yuan@senaicni.com.br|VACUUM END  |{status -> COMPLETED}                                                                                                                                                                |null|{3307471690888693}|0429-204748-rasps399|5          |SnapshotIsolation|true         |{numDeletedFiles -> 0, numVacuumedDirectories -> 1}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |null        |Databricks-Runtime/12.2.x-scala2.12|\n|5      |2024-10-25 23:05:31|3466497405212432|zhang.yuan@senaicni.com.br|VACUUM START|{retentionCheckEnabled -> true, defaultRetentionMillis -> 604800000, specifiedRetentionMillis -> 604800000}                                                                          |null|{3307471690888693}|0429-204748-rasps399|4          |SnapshotIsolation|true         |{numFilesToDelete -> 0, sizeOfDataToDelete -> 0}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |null        |Databricks-Runtime/12.2.x-scala2.12|\n|4      |2024-10-25 23:04:37|3466497405212432|zhang.yuan@senaicni.com.br|VACUUM END  |{status -> COMPLETED}                                                                                                                                                                |null|{3307471690888693}|0429-204748-rasps399|3          |SnapshotIsolation|true         |{numDeletedFiles -> 0, numVacuumedDirectories -> 1}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |null        |Databricks-Runtime/12.2.x-scala2.12|\n|3      |2024-10-25 23:04:35|3466497405212432|zhang.yuan@senaicni.com.br|VACUUM START|{retentionCheckEnabled -> true, defaultRetentionMillis -> 604800000, specifiedRetentionMillis -> 604800000}                                                                          |null|{3307471690888693}|0429-204748-rasps399|2          |SnapshotIsolation|true         |{numFilesToDelete -> 0, sizeOfDataToDelete -> 0}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |null        |Databricks-Runtime/12.2.x-scala2.12|\n|2      |2024-10-25 22:53:57|3466497405212432|zhang.yuan@senaicni.com.br|MERGE       |{predicate -> [\"(name#752627 = name#752605)\"], matchedPredicates -> [{\"actionType\":\"update\"}], notMatchedPredicates -> [{\"actionType\":\"insert\"}], notMatchedBySourcePredicates -> []}|null|{3307471690888693}|0429-204748-rasps399|1          |WriteSerializable|false        |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 2, numTargetBytesAdded -> 1616, numTargetBytesRemoved -> 1616, numTargetDeletionVectorsAdded -> 0, numTargetRowsMatchedUpdated -> 2, executionTimeMs -> 2804, materializeSourceTimeMs -> 182, numTargetRowsInserted -> 0, numTargetRowsMatchedDeleted -> 0, scanTimeMs -> 1699, numTargetRowsUpdated -> 2, numOutputRows -> 2, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 2, numTargetFilesRemoved -> 2, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 838}|null        |Databricks-Runtime/12.2.x-scala2.12|\n|1      |2024-10-25 22:47:11|3466497405212432|zhang.yuan@senaicni.com.br|WRITE       |{mode -> Overwrite, partitionBy -> []}                                                                                                                                               |null|{3307471690888693}|0429-204748-rasps399|0          |WriteSerializable|false        |{numFiles -> 3, numOutputRows -> 3, numOutputBytes -> 2410}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |null        |Databricks-Runtime/12.2.x-scala2.12|\n|0      |2024-10-25 22:43:54|3466497405212432|zhang.yuan@senaicni.com.br|WRITE       |{mode -> Overwrite, partitionBy -> []}                                                                                                                                               |null|{3307471690888693}|0429-204748-rasps399|null       |WriteSerializable|false        |{numFiles -> 3, numOutputRows -> 3, numOutputBytes -> 2410}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |null        |Databricks-Runtime/12.2.x-scala2.12|\n+-------+-------------------+----------------+--------------------------+------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "#################################\n",
    "### Monitoramento e Auditoria ###\n",
    "#################################\n",
    "\n",
    "# O Delta Lake permite que você monitore as operações e mantenha um histórico de alterações para auditoria.\n",
    "# python\n",
    "# Copiar código\n",
    "# Consultar o histórico de operações\n",
    "\n",
    "history_df = delta_table.history(10)  # últimos 10 comandos\n",
    "history_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfa9622f-b10a-473a-a25e-875b72b8e235",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th><th>engineInfo</th></tr></thead><tbody><tr><td>7</td><td>2024-10-25T23:06:52.000+0000</td><td>3466497405212432</td><td>zhang.yuan@senaicni.com.br</td><td>WRITE</td><td>Map(mode -> Append, partitionBy -> [])</td><td>null</td><td>List(3307471690888693)</td><td>0429-204748-rasps399</td><td>6</td><td>WriteSerializable</td><td>true</td><td>Map(numFiles -> 3, numOutputRows -> 3, numOutputBytes -> 3064)</td><td>null</td><td>Databricks-Runtime/12.2.x-scala2.12</td></tr><tr><td>6</td><td>2024-10-25T23:05:33.000+0000</td><td>3466497405212432</td><td>zhang.yuan@senaicni.com.br</td><td>VACUUM END</td><td>Map(status -> COMPLETED)</td><td>null</td><td>List(3307471690888693)</td><td>0429-204748-rasps399</td><td>5</td><td>SnapshotIsolation</td><td>true</td><td>Map(numDeletedFiles -> 0, numVacuumedDirectories -> 1)</td><td>null</td><td>Databricks-Runtime/12.2.x-scala2.12</td></tr><tr><td>5</td><td>2024-10-25T23:05:31.000+0000</td><td>3466497405212432</td><td>zhang.yuan@senaicni.com.br</td><td>VACUUM START</td><td>Map(retentionCheckEnabled -> true, defaultRetentionMillis -> 604800000, specifiedRetentionMillis -> 604800000)</td><td>null</td><td>List(3307471690888693)</td><td>0429-204748-rasps399</td><td>4</td><td>SnapshotIsolation</td><td>true</td><td>Map(numFilesToDelete -> 0, sizeOfDataToDelete -> 0)</td><td>null</td><td>Databricks-Runtime/12.2.x-scala2.12</td></tr><tr><td>4</td><td>2024-10-25T23:04:37.000+0000</td><td>3466497405212432</td><td>zhang.yuan@senaicni.com.br</td><td>VACUUM END</td><td>Map(status -> COMPLETED)</td><td>null</td><td>List(3307471690888693)</td><td>0429-204748-rasps399</td><td>3</td><td>SnapshotIsolation</td><td>true</td><td>Map(numDeletedFiles -> 0, numVacuumedDirectories -> 1)</td><td>null</td><td>Databricks-Runtime/12.2.x-scala2.12</td></tr><tr><td>3</td><td>2024-10-25T23:04:35.000+0000</td><td>3466497405212432</td><td>zhang.yuan@senaicni.com.br</td><td>VACUUM START</td><td>Map(retentionCheckEnabled -> true, defaultRetentionMillis -> 604800000, specifiedRetentionMillis -> 604800000)</td><td>null</td><td>List(3307471690888693)</td><td>0429-204748-rasps399</td><td>2</td><td>SnapshotIsolation</td><td>true</td><td>Map(numFilesToDelete -> 0, sizeOfDataToDelete -> 0)</td><td>null</td><td>Databricks-Runtime/12.2.x-scala2.12</td></tr><tr><td>2</td><td>2024-10-25T22:53:57.000+0000</td><td>3466497405212432</td><td>zhang.yuan@senaicni.com.br</td><td>MERGE</td><td>Map(predicate -> [\"(name#752627 = name#752605)\"], matchedPredicates -> [{\"actionType\":\"update\"}], notMatchedPredicates -> [{\"actionType\":\"insert\"}], notMatchedBySourcePredicates -> [])</td><td>null</td><td>List(3307471690888693)</td><td>0429-204748-rasps399</td><td>1</td><td>WriteSerializable</td><td>false</td><td>Map(numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 2, numTargetBytesAdded -> 1616, numTargetBytesRemoved -> 1616, numTargetDeletionVectorsAdded -> 0, numTargetRowsMatchedUpdated -> 2, executionTimeMs -> 2804, materializeSourceTimeMs -> 182, numTargetRowsInserted -> 0, numTargetRowsMatchedDeleted -> 0, scanTimeMs -> 1699, numTargetRowsUpdated -> 2, numOutputRows -> 2, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 2, numTargetFilesRemoved -> 2, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 838)</td><td>null</td><td>Databricks-Runtime/12.2.x-scala2.12</td></tr><tr><td>1</td><td>2024-10-25T22:47:11.000+0000</td><td>3466497405212432</td><td>zhang.yuan@senaicni.com.br</td><td>WRITE</td><td>Map(mode -> Overwrite, partitionBy -> [])</td><td>null</td><td>List(3307471690888693)</td><td>0429-204748-rasps399</td><td>0</td><td>WriteSerializable</td><td>false</td><td>Map(numFiles -> 3, numOutputRows -> 3, numOutputBytes -> 2410)</td><td>null</td><td>Databricks-Runtime/12.2.x-scala2.12</td></tr><tr><td>0</td><td>2024-10-25T22:43:54.000+0000</td><td>3466497405212432</td><td>zhang.yuan@senaicni.com.br</td><td>WRITE</td><td>Map(mode -> Overwrite, partitionBy -> [])</td><td>null</td><td>List(3307471690888693)</td><td>0429-204748-rasps399</td><td>null</td><td>WriteSerializable</td><td>false</td><td>Map(numFiles -> 3, numOutputRows -> 3, numOutputBytes -> 2410)</td><td>null</td><td>Databricks-Runtime/12.2.x-scala2.12</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         7,
         "2024-10-25T23:06:52.000+0000",
         "3466497405212432",
         "zhang.yuan@senaicni.com.br",
         "WRITE",
         {
          "mode": "Append",
          "partitionBy": "[]"
         },
         null,
         [
          "3307471690888693"
         ],
         "0429-204748-rasps399",
         6,
         "WriteSerializable",
         true,
         {
          "numFiles": "3",
          "numOutputBytes": "3064",
          "numOutputRows": "3"
         },
         null,
         "Databricks-Runtime/12.2.x-scala2.12"
        ],
        [
         6,
         "2024-10-25T23:05:33.000+0000",
         "3466497405212432",
         "zhang.yuan@senaicni.com.br",
         "VACUUM END",
         {
          "status": "COMPLETED"
         },
         null,
         [
          "3307471690888693"
         ],
         "0429-204748-rasps399",
         5,
         "SnapshotIsolation",
         true,
         {
          "numDeletedFiles": "0",
          "numVacuumedDirectories": "1"
         },
         null,
         "Databricks-Runtime/12.2.x-scala2.12"
        ],
        [
         5,
         "2024-10-25T23:05:31.000+0000",
         "3466497405212432",
         "zhang.yuan@senaicni.com.br",
         "VACUUM START",
         {
          "defaultRetentionMillis": "604800000",
          "retentionCheckEnabled": "true",
          "specifiedRetentionMillis": "604800000"
         },
         null,
         [
          "3307471690888693"
         ],
         "0429-204748-rasps399",
         4,
         "SnapshotIsolation",
         true,
         {
          "numFilesToDelete": "0",
          "sizeOfDataToDelete": "0"
         },
         null,
         "Databricks-Runtime/12.2.x-scala2.12"
        ],
        [
         4,
         "2024-10-25T23:04:37.000+0000",
         "3466497405212432",
         "zhang.yuan@senaicni.com.br",
         "VACUUM END",
         {
          "status": "COMPLETED"
         },
         null,
         [
          "3307471690888693"
         ],
         "0429-204748-rasps399",
         3,
         "SnapshotIsolation",
         true,
         {
          "numDeletedFiles": "0",
          "numVacuumedDirectories": "1"
         },
         null,
         "Databricks-Runtime/12.2.x-scala2.12"
        ],
        [
         3,
         "2024-10-25T23:04:35.000+0000",
         "3466497405212432",
         "zhang.yuan@senaicni.com.br",
         "VACUUM START",
         {
          "defaultRetentionMillis": "604800000",
          "retentionCheckEnabled": "true",
          "specifiedRetentionMillis": "604800000"
         },
         null,
         [
          "3307471690888693"
         ],
         "0429-204748-rasps399",
         2,
         "SnapshotIsolation",
         true,
         {
          "numFilesToDelete": "0",
          "sizeOfDataToDelete": "0"
         },
         null,
         "Databricks-Runtime/12.2.x-scala2.12"
        ],
        [
         2,
         "2024-10-25T22:53:57.000+0000",
         "3466497405212432",
         "zhang.yuan@senaicni.com.br",
         "MERGE",
         {
          "matchedPredicates": "[{\"actionType\":\"update\"}]",
          "notMatchedBySourcePredicates": "[]",
          "notMatchedPredicates": "[{\"actionType\":\"insert\"}]",
          "predicate": "[\"(name#752627 = name#752605)\"]"
         },
         null,
         [
          "3307471690888693"
         ],
         "0429-204748-rasps399",
         1,
         "WriteSerializable",
         false,
         {
          "executionTimeMs": "2804",
          "materializeSourceTimeMs": "182",
          "numOutputRows": "2",
          "numSourceRows": "2",
          "numTargetBytesAdded": "1616",
          "numTargetBytesRemoved": "1616",
          "numTargetChangeFilesAdded": "0",
          "numTargetDeletionVectorsAdded": "0",
          "numTargetDeletionVectorsRemoved": "0",
          "numTargetFilesAdded": "2",
          "numTargetFilesRemoved": "2",
          "numTargetRowsCopied": "0",
          "numTargetRowsDeleted": "0",
          "numTargetRowsInserted": "0",
          "numTargetRowsMatchedDeleted": "0",
          "numTargetRowsMatchedUpdated": "2",
          "numTargetRowsNotMatchedBySourceDeleted": "0",
          "numTargetRowsNotMatchedBySourceUpdated": "0",
          "numTargetRowsUpdated": "2",
          "rewriteTimeMs": "838",
          "scanTimeMs": "1699"
         },
         null,
         "Databricks-Runtime/12.2.x-scala2.12"
        ],
        [
         1,
         "2024-10-25T22:47:11.000+0000",
         "3466497405212432",
         "zhang.yuan@senaicni.com.br",
         "WRITE",
         {
          "mode": "Overwrite",
          "partitionBy": "[]"
         },
         null,
         [
          "3307471690888693"
         ],
         "0429-204748-rasps399",
         0,
         "WriteSerializable",
         false,
         {
          "numFiles": "3",
          "numOutputBytes": "2410",
          "numOutputRows": "3"
         },
         null,
         "Databricks-Runtime/12.2.x-scala2.12"
        ],
        [
         0,
         "2024-10-25T22:43:54.000+0000",
         "3466497405212432",
         "zhang.yuan@senaicni.com.br",
         "WRITE",
         {
          "mode": "Overwrite",
          "partitionBy": "[]"
         },
         null,
         [
          "3307471690888693"
         ],
         "0429-204748-rasps399",
         null,
         "WriteSerializable",
         false,
         {
          "numFiles": "3",
          "numOutputBytes": "2410",
          "numOutputRows": "3"
         },
         null,
         "Databricks-Runtime/12.2.x-scala2.12"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "version",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "userId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "userName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "operation",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "operationParameters",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "job",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"jobId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobRunId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"runId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobOwnerId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"triggerType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "notebook",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"notebookId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "clusterId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "readVersion",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "isolationLevel",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "isBlindAppend",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "operationMetrics",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "userMetadata",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "engineInfo",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62579336-8ec8-407a-9eb2-b5cef35c720c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea2c326e-a916-43b0-90ba-8f2d6efd5ace",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "898c9a28-d42a-4780-8a93-03ae912320a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Se precisar\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Inicializar a SparkSession com suporte a Delta\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaLakeStreamingExample\") \\\n",
    "    .config(\"spark.sql.extensions\", \"delta.sql.DeltaSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "605db1d2-5ceb-413e-8fa6-84ed6563eb67",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Se precisar\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a SparkSession with support for Delta Lake\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaLakeStreamingExample\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb254e4e-b08c-4801-a172-51bff74fe689",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#######################\n",
    "### Streaming_input ###\n",
    "#######################\n",
    "\n",
    "# Create a list of tuples with the data\n",
    "data = [(\"Alice\", 30), (\"Bob\", 25), (\"Cathy\", 27)]\n",
    "\n",
    "# Define the schema\n",
    "columns = [\"name\", \"age\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df_test = spark.createDataFrame(data, schema=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e120c19d-87ad-488c-8f53-e3a5059c2f54",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>age</th></tr></thead><tbody><tr><td>Alice</td><td>30</td></tr><tr><td>Bob</td><td>25</td></tr><tr><td>Cathy</td><td>27</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Alice",
         30
        ],
        [
         "Bob",
         25
        ],
        [
         "Cathy",
         27
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_test.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26ea6f41-7f56-44bf-8711-e1e481d2162f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[11]: True"
     ]
    }
   ],
   "source": [
    "# Make a Directory:\n",
    "dbutils.fs.mkdirs(\"dbfs:/Streaming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c467b8c9-303f-49da-91b2-a745ab84034c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[9]: True"
     ]
    }
   ],
   "source": [
    "# Delete Files:\n",
    "dbutils.fs.rm(\"dbfs:/Streaming\", True)  # True to delete recursively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08e54c87-7108-472b-8acd-b42bbfc3819f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Suponha que df_test seja seu DataFrame\n",
    "df_test.coalesce(1).write.format(\"csv\").mode(\"overwrite\").option(\"header\", \"true\").save(\"dbfs:/Streaming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0d307f5-94d8-4346-9b74-ed988e957bd8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5a1ec5f-127a-4c7a-af59-f92135ed2d13",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "### Criar uma fonte de dados de streaming ###\n",
    "#############################################\n",
    "\n",
    "# Para fins de exemplo, você pode usar um diretório onde os arquivos CSV serão colocados\n",
    "# input_path = \"/mnt/streaming_input\"\n",
    "input_path = \"dbfs:/Streaming\"\n",
    "\n",
    "# Suponha que os arquivos CSV tenham as colunas \"name\" e \"age\"\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Define the schema of the CSV files\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Use the defined schema in the readStream operation\n",
    "streaming_df = spark.readStream \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "058ab0f4-d5b0-4b87-b333-60543df62cd7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>age</th></tr></thead><tbody><tr><td>Alice</td><td>30</td></tr><tr><td>Bob</td><td>25</td></tr><tr><td>Cathy</td><td>27</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Alice",
         30
        ],
        [
         "Bob",
         25
        ],
        [
         "Cathy",
         27
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "streaming_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bad39683-1394-4ec4-a2b3-f213bf7afec3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "########################################\n",
    "### Processar e Gravar em Delta Lake ###\n",
    "########################################\n",
    "\n",
    "# Processar e Gravar em Delta Lake\n",
    "# Agora, vamos processar os dados recebidos e gravá-los em uma tabela Delta.\n",
    "# python\n",
    "# Copiar código\n",
    "\n",
    "# Especificar o caminho da tabela Delta\n",
    "# delta_table_path = \"/mnt/delta_table_streaming\"\n",
    "delta_table_path = \"dbfs:/Delta_table_streaming\"\n",
    "\n",
    "# Escrever os dados em um formato Delta\n",
    "query = streaming_df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"dbfs:/Delta_table_streaming\") \\\n",
    "    .start(delta_table_path)\n",
    "# .option(\"checkpointLocation\", \"/mnt/checkpoints\") \\\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e0a552d-db1b-449c-b21d-cf465b66c1e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##############################################\n",
    "### Ler Dados da Tabela Delta em Streaming ###\n",
    "##############################################\n",
    "\n",
    "# Agora que temos os dados sendo gravados na tabela Delta, podemos criar uma consulta para ler esses dados em tempo real.\n",
    "# python\n",
    "# Copiar código\n",
    "# Ler os dados da tabela Delta em streaming\n",
    "delta_streaming_df = spark.readStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(delta_table_path)\n",
    "\n",
    "# Escrever os dados lidos na saída do console\n",
    "query_console = delta_streaming_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query_console.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e063fda-b166-467f-af9f-453d5ed6e15c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "### Simulando Dados de Entrada ###\n",
    "##################################\n",
    "\n",
    "# Para testar este exemplo, você pode usar o netcat (ou nc) para enviar dados ao socket. Abra um terminal e execute:\n",
    "# bash\n",
    "# Copiar código\n",
    "# nc -lk 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c01e8bfd-846c-4b0e-9c1b-dad1eff77d26",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Delta_Format",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
